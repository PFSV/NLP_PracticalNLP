{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHnDE3OC6lrl"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install datasets   \n",
        "\n",
        "# Import packages\n",
        "from datasets import load_dataset   # Huggingface 데이터셋 패키지 import\n",
        "from google.colab import drive\n",
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score # Accuracy 측정 함수 import\n",
        "\n",
        "##### konlpy 다운로드, 코드 상단에서 KoNLPy, Mecab 설치 명령어 실행\n",
        "# connect google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download konlpy\n",
        "%cd ./drive/MyDrive/Colab\\ Notebooks/\n",
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git \n",
        "%cd ./Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab_light_220429.sh \n",
        "from konlpy.tag import Mecab  # KoNLPy를 통해 Mecab 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### 데이터 다운로드\n",
        "data = load_dataset(\"sepidmnorozy/Korean_sentiment\")    # 데이터 다운로드"
      ],
      "metadata": {
        "id": "5MgbTADy06BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Mecab 동작 확인코드 실행\n",
        "# from konlpy.tag import Mecab  # KoNLPy를 통해 Mecab 패키지 import\n",
        "# mecab = Mecab() \n",
        "\n",
        "# sentence = \"실용자연어처리 실습 진행중 이에요. 수업을 시작 할게요\"\n",
        "# print(mecab.morphs(sentence)) # 형태소 출력\n",
        "# print(mecab.nouns(sentence))  # 명사 출력\n",
        "# print(mecab.pos(sentence))    # 형태소와 형태소 태그 출력"
      ],
      "metadata": {
        "id": "o2TRw3m1ZIDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Word embedding 확인\n",
        "# load word2vec embedding\n",
        "embedding_model = gensim.models.Word2Vec.load('/content/drive/MyDrive/Colab Notebooks/word2vec/word2vec')\n",
        "emb_words = embedding_model.wv.index_to_key\n",
        "\n",
        "print(emb_words[:10])"
      ],
      "metadata": {
        "id": "t5qNkqVzPGvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "##### 데이터 구조 훑어보기\n",
        "# print(\"Data type: \", type(data))    # 데이터 타입 확인\n",
        "# print(\"Data structure: \", data)     # 데이터 구조 확인\n",
        "# print(\"Data keys: \", data.keys())   # 데이터 키 확인\n",
        "\n",
        "print(data['train'][0])   # 실제 데이터 확인"
      ],
      "metadata": {
        "id": "8_NM2aMmCjkA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 테스트 세트 만들기\n",
        "train_data = data['train']\n",
        "dev_data = data['validation']\n",
        "test_data = data['test']\n",
        "\n",
        "#print(train_data)\n",
        "#print(dev_data)\n",
        "#print(test_data)\n",
        "\n",
        "# mecab 형태소 분석기 사용\n",
        "mecab = Mecab() \n",
        "\n",
        "# 형태소 단위로 Tokenization 된 텍스트를 {train/dev/test}_data에 저장 \n",
        "train_data = train_data.map(lambda example: {'label': example['label'], 'text': ' '.join(mecab.morphs(example['text']))})\n",
        "dev_data = dev_data.map(lambda example: {'label': example['label'], 'text': ' '.join(mecab.morphs(example['text']))})\n",
        "test_data = test_data.map(lambda example: {'label': example['label'], 'text': ' '.join(mecab.morphs(example['text']))})\n",
        "\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "IqbCh4yMCiEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 임베딩 사용하기 \n",
        "### 룩업테이블 정의\n",
        "emb_words = embedding_model.wv.index_to_key\n",
        "emb_vectors = embedding_model.wv.vectors\n",
        "emb_dim = embedding_model.vector_size\n",
        "emb_stoi = {}\n",
        "max_seq_len = 30\n",
        "\n",
        "# <pad> -> 0, <unk> -> 1\n",
        "emb_stoi['<pad>'] = 0\n",
        "emb_stoi['<unk>'] = 1\n",
        "\n",
        "print(len(emb_vectors))\n",
        "pad_vector = np.zeros(shape=(1, emb_dim))\n",
        "unk_vector = np.zeros(shape=(1, emb_dim))\n",
        "emb_vectors = np.concatenate((pad_vector, unk_vector, emb_vectors), axis=0)\n",
        "print(len(emb_vectors))\n",
        "\n",
        "for i, word in enumerate(emb_words):\n",
        "  emb_stoi[word] = i + 2\n",
        "\n",
        "emb_vectors = torch.FloatTensor(emb_vectors)\n",
        "\n",
        "\n",
        "### 인덱스 변환 \n",
        "def text_to_index(input_data, stoi, max_seq_len):\n",
        "  all_index = []\n",
        "\n",
        "  for sample in input_data:\n",
        "    index_list = []\n",
        "    for word in sample['text'].split():\n",
        "      if word in stoi.keys(): # 단어가 lookup table에 검색 가능한 경우\n",
        "        index_list.append(stoi[word])\n",
        "      else: # 단어가 lookup table에 검색 불가능한 경우\n",
        "        index_list.append(stoi['<unk>'])\n",
        "\n",
        "    # Padding: 샘플의 길이가 고정 크기의 최대 길이 (L)보다 작은 경우, pad 인덱스를 추가\n",
        "    if max_seq_len > len(index_list):\n",
        "      index_list = index_list + [stoi['<pad>']] * (max_seq_len - len(index_list))\n",
        "    else: # 샘플의 길이가 고정 크기의 최대 길이 (L)보다 큰 경우, 고정 크기까지만 처리\n",
        "      index_list = index_list[:max_seq_len]\n",
        "    all_index.append(index_list)\n",
        "\n",
        "  return all_index\n",
        "\n",
        "# train, dev, test data들을 index로 변환\n",
        "train_index = text_to_index(train_data, emb_stoi, max_seq_len)\n",
        "dev_index = text_to_index(dev_data, emb_stoi, max_seq_len)\n",
        "test_index = text_to_index(test_data, emb_stoi, max_seq_len)\n",
        "\n",
        "# 강의자료 page 20 구현 후 출력 결과 확인\n",
        "# print(train_index[0])\n",
        "# print(emb_words[717-2], emb_words[4526-2], emb_words[5861-2], emb_words[16-2],emb_words[100-2])\n",
        "# print(train_index[100])"
      ],
      "metadata": {
        "id": "-yvLmeetCGlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 훈련세트에서 훈련하고 평가하기\n",
        "\n",
        "# randomness 제거\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# MLP 모델의 아키텍쳐(구조) 정의\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, max_seq_len):\n",
        "    super(MLP, self).__init__()\n",
        "    # embedding layer\n",
        "    self.word_embedding = nn.Embedding.from_pretrained(embeddings=emb_vectors)\n",
        "    self.input_size = input_size\n",
        "    self.max_seq_len = max_seq_len\n",
        "    \n",
        "    # 각 층에 대한 정의\n",
        "    self.layer1 = nn.Linear(max_seq_len * input_size, hidden_size)\n",
        "    self.layer2 = nn.Linear(hidden_size, int(0.5*hidden_size))\n",
        "    self.layer3 = nn.Linear(int(0.5*hidden_size), output_size)\n",
        "    # 활성화 함수\n",
        "    self.gelu = nn.GELU()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 실제(입력) 데이터: x\n",
        "    # x를 가지고 순전파를 진행\n",
        "    word_emb = self.word_embedding(torch.cuda.LongTensor(x))\n",
        "    word_emb_new = word_emb.view((-1, self.max_seq_len * self.input_size))\n",
        "\n",
        "    out1 = self.layer1(word_emb_new) #(sum)\n",
        "    out2 = self.gelu(out1) # hidden_size\n",
        "    out3 = self.layer2(out2)\n",
        "    out4 = self.gelu(out3)\n",
        "    out5 = self.layer3(out4)\n",
        "    out6 = self.softmax(out5)\n",
        "\n",
        "    return out6\n",
        "\n",
        "'''\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(MLP, self).__init__()\n",
        "    self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "    self.layer2 = nn.Linear(hidden_size, int(0.5*hidden_size))\n",
        "    self.layer3 = nn.Linear(int(0.5*hidden_size), int(2*hidden_size))\n",
        "    self.layer4 = nn.Linear(int(2*hidden_size), hidden_size)\n",
        "    self.layer5 = nn.Linear(hidden_size, output_size)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = self.layer1(x)\n",
        "    out2 = self.gelu(out1)\n",
        "    out3 = self.layer2(out2)\n",
        "    out4 = self.gelu(out3)\n",
        "    out5 = self.layer3(out4)\n",
        "    out6 = self.gelu(out5)\n",
        "    out7 = self.layer4(out6)\n",
        "    out8 = self.gelu(out7)\n",
        "    out9 = self.layer5(out8)\n",
        "    out10 = self.softmax(out9)\n",
        "\n",
        "    return out10\n",
        "'''\n",
        "# 하이퍼파라미터 셋팅\n",
        "input_size = emb_dim #len(vectorizer.vocabulary_) # input size\n",
        "hidden_size = 100 # hidden size\n",
        "output_size = 2 # output size is 2 (positive/negative)\n",
        "learning_rate = 0.00001\n",
        "batch_size = 128\n",
        "num_epochs = 20\n",
        "\n",
        "# 모델 초기화\n",
        "model = MLP(input_size, hidden_size, output_size, max_seq_len)\n",
        "device = torch.device(\"cuda\") # use GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# optimizer, loss function 정의\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# dev_vectors와 dev_data['label']을 텐서 자료형으로 변환\n",
        "dev_tensors = dev_index #torch.cuda.FloatTensor(dev_vectors.toarray(), device=device)\n",
        "dev_labels = torch.tensor(dev_data['label'], dtype=torch.long, device=device)\n",
        "\n",
        "# 학습 시작 (총 num_epochs 만큼)\n",
        "for epoch in range(num_epochs):\n",
        "  # model을 학습모드로 만든다.\n",
        "  model.train()\n",
        "  \n",
        "  # 학습을 하면서 보고싶은 숫자 (학습이 잘되는지 확인)\n",
        "  epoch_loss = 0\n",
        "  best_accuracy = 0\n",
        "\n",
        "  # train_vectors를 텐서 자료형으로 변환\n",
        "  train_tensors = train_index #torch.cuda.FloatTensor(train_vectors.toarray(), device=device)\n",
        "\n",
        "  # batch size 단위로 학습 진행\n",
        "  for i in range(0, len(train_tensors), batch_size):\n",
        "\n",
        "    # batch 단위 데이터 생성\n",
        "    batch_data = train_tensors[i:i+batch_size] # batch size 크기의 데이터가 batch_data\n",
        "    batch_labels = torch.tensor(train_data['label'][i:i+batch_size], device=device)\n",
        "\n",
        "    # 1. 순전파\n",
        "    outputs = model(batch_data)\n",
        "    # 2. 오차 계산\n",
        "    loss = loss_function(outputs, batch_labels)\n",
        "    # 3. 역전파\n",
        "    optimizer.zero_grad()\n",
        "    # 4. 가중치 업데이트\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # epoch loss\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  # 매 epoch마다 dev 성능 측정\n",
        "  # 모델을 평가하는 모드로 셋팅\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    dev_outputs = model(dev_tensors) # dev 데이터의 순전파\n",
        "    dev_preds = torch.argmax(dev_outputs, axis=1)\n",
        "    dev_accuracy = torch.sum(dev_preds == dev_labels).item() / len(dev_labels)\n",
        "\n",
        "    # save best model on dev data\n",
        "    if dev_accuracy > best_accuracy:\n",
        "      best_model = model\n",
        "      best_accuracy = dev_accuracy\n",
        "\n",
        "  print(f\"Epoch {epoch+1}, Accuracy: {dev_accuracy} , loss: {epoch_loss/len(train_tensors)}\")"
      ],
      "metadata": {
        "id": "u_hMc3yACbYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 테스트 세트로 시스템 평가하기\n",
        "final_model = best_model # MLP 모델\n",
        "test_tensors = test_index #torch.cuda.FloatTensor(test_vectors.toarray(), device=device)\n",
        "test_outputs = final_model(test_tensors) # 최종 모델로 test 데이터 예측\n",
        "pred_results = torch.argmax(test_outputs, axis=1)\n",
        "accuracy = accuracy_score(test_data['label'], pred_results.tolist()) # 정확도 측정\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100)) # 정확도 출력"
      ],
      "metadata": {
        "id": "caRvzXUtCUrv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}