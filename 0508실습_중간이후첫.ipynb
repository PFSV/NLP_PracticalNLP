{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHnDE3OC6lrl"
      },
      "outputs": [],
      "source": [
        "##### 데이터 다운로드\n",
        "!pip install datasets   # Package install\n",
        "\n",
        "from datasets import load_dataset   # Huggingface 데이터셋 패키지 import\n",
        "data = load_dataset(\"sepidmnorozy/Korean_sentiment\")    # 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### 코드 상단에서 KoNLPy, Mecab 설치 명령어 실행\n",
        "# connect google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download konlpy\n",
        "%cd ./drive/MyDrive/Colab\\ Notebooks/\n",
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git \n",
        "%cd ./Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab_light_220429.sh "
      ],
      "metadata": {
        "id": "dAO2bWH5ZCzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Mecab 동작 확인코드 실행\n",
        "# from konlpy.tag import Mecab  # KoNLPy를 통해 Mecab 패키지 import\n",
        "# mecab = Mecab() \n",
        "\n",
        "# sentence = \"실용자연어처리 실습 진행중 이에요. 수업을 시작 할게요\"\n",
        "# print(mecab.morphs(sentence)) # 형태소 출력\n",
        "# print(mecab.nouns(sentence))  # 명사 출력\n",
        "# print(mecab.pos(sentence))    # 형태소와 형태소 태그 출력"
      ],
      "metadata": {
        "id": "o2TRw3m1ZIDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 데이터 구조 훑어보기\n",
        "# print(\"Data type: \", type(data))    # 데이터 타입 확인\n",
        "# print(\"Data structure: \", data)     # 데이터 구조 확인\n",
        "# print(\"Data keys: \", data.keys())   # 데이터 키 확인\n",
        "\n",
        "print(data['train'][0])   # 실제 데이터 확인"
      ],
      "metadata": {
        "id": "8_NM2aMmCjkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 테스트 세트 만들기\n",
        "train_data = data['train']\n",
        "dev_data = data['validation']\n",
        "test_data = data['test']\n",
        "\n",
        "#print(train_data)\n",
        "#print(dev_data)\n",
        "#print(test_data)\n",
        "\n",
        "# mecab 형태소 분석기 사용\n",
        "from konlpy.tag import Mecab  # KoNLPy를 통해 Mecab 패키지 import\n",
        "mecab = Mecab() \n",
        "\n",
        "# 형태소 단위로 Tokenization 된 텍스트를 {train/dev/test}_data에 저장 \n",
        "train_data = train_data.map(lambda example: {'label': example['label'], 'text': ' '.join(mecab.morphs(example['text']))})\n",
        "dev_data = dev_data.map(lambda example: {'label': example['label'], 'text': ' '.join(mecab.morphs(example['text']))})\n",
        "test_data = test_data.map(lambda example: {'label': example['label'], 'text': ' '.join(mecab.morphs(example['text']))})\n",
        "\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "IqbCh4yMCiEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 텍스트 데이터 특성\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.hist(data['train']['label'], color='red')\n",
        "# plt.show()\n",
        "# plt.hist(data['validation']['label'], color='blue')\n",
        "# plt.show()\n",
        "# plt.hist(data['test']['label'], color='green')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "NSi7J4fvCfRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 텍스트와 범주형 특성 다루기 (Bag-of-words 방식으로 벡터화 하는 법 구현)\n",
        "from sklearn.feature_extraction.text import CountVectorizer       # CountVectorizer: Bag of words 벡터화 구현을 하기 위한 클래스\n",
        "\n",
        "vectorizer = CountVectorizer(strip_accents='unicode', token_pattern=r\"(?u)\\b\\w\\w+\\b|'\\w+\")      # 데이터를 벡터화 해주는 모델 \n",
        "vectorizer.fit(train_data['text'])  # 텍스트 문서 모음을 토큰 수의 행렬로 변환\n",
        "\n",
        "print(vectorizer.vocabulary_)       # 텍스트 문서에 나타난 어휘의 집합을 출력\n",
        "print(len(vectorizer.vocabulary_))  # 텍스트 문서에 나타난 어휘 집합의 길이를 출력\n",
        "\n",
        "train_vectors = vectorizer.transform(train_data['text'])    # 학습 데이터를 숫자로 변환\n",
        "dev_vectors = vectorizer.transform(dev_data['text'])        # 검증 데이터를 숫자로 변환\n",
        "test_vectors = vectorizer.transform(test_data['text'])      # 테스트 데이터를 숫자로 변환\n",
        "\n",
        "# print(train_vectors)      # transform 된 결과 확인\n",
        "\n",
        "\n",
        "##### 텍스트와 범주형 특성 다루기 (Bag-of-words 방식으로 벡터화 한 것 확인하는 방법 구현)\n",
        "sample_num = -1                            # 확인하고 싶은 샘플 번호\n",
        "sample_origin = train_data[sample_num]    # 확인하고 싶은 샘플의 원래 문장\n",
        "sample_transform = train_vectors[sample_num]    # 확인하고 싶은 샘플의 transform된 결과\n",
        "sample_inverse_transform = vectorizer.inverse_transform(sample_transform) # 확인하고 싶은 샘플의 transform된 결과를 다시 단어의 조합으로 바꾼 결과\n",
        "print(\"Original Input:{}\\nTransformed: {}\\nInv-transformed: {}\".format(sample_origin, sample_transform, sample_inverse_transform))    # 출력"
      ],
      "metadata": {
        "id": "uVsAWEB2Cd2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 훈련세트에서 훈련하고 평가하기\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# randomness 제거\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# MLP 모델의 아키텍쳐(구조) 정의\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(MLP, self).__init__()\n",
        "    # 각 층에 대한 정의\n",
        "    self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "    self.layer2 = nn.Linear(hidden_size, int(0.5*hidden_size))\n",
        "    self.layer3 = nn.Linear(int(0.5*hidden_size), output_size)\n",
        "    # 활성화 함수\n",
        "    self.gelu = nn.GELU()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 실제(입력) 데이터: x\n",
        "    # x를 가지고 순전파를 진행\n",
        "    out1 = self.layer1(x) #(sum)\n",
        "    out2 = self.gelu(out1) # hidden_size\n",
        "    out3 = self.layer2(out2)\n",
        "    out4 = self.gelu(out3)\n",
        "    out5 = self.layer3(out4)\n",
        "    out6 = self.softmax(out5)\n",
        "\n",
        "    return out6\n",
        "\n",
        "'''\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(MLP, self).__init__()\n",
        "    self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "    self.layer2 = nn.Linear(hidden_size, int(0.5*hidden_size))\n",
        "    self.layer3 = nn.Linear(int(0.5*hidden_size), int(2*hidden_size))\n",
        "    self.layer4 = nn.Linear(int(2*hidden_size), hidden_size)\n",
        "    self.layer5 = nn.Linear(hidden_size, output_size)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = self.layer1(x)\n",
        "    out2 = self.gelu(out1)\n",
        "    out3 = self.layer2(out2)\n",
        "    out4 = self.gelu(out3)\n",
        "    out5 = self.layer3(out4)\n",
        "    out6 = self.gelu(out5)\n",
        "    out7 = self.layer4(out6)\n",
        "    out8 = self.gelu(out7)\n",
        "    out9 = self.layer5(out8)\n",
        "    out10 = self.softmax(out9)\n",
        "\n",
        "    return out10\n",
        "'''\n",
        "# 하이퍼파라미터 셋팅\n",
        "input_size = len(vectorizer.vocabulary_) # input size\n",
        "hidden_size = 100 # hidden size\n",
        "output_size = 2 # output size is 2 (positive/negative)\n",
        "learning_rate = 0.00001\n",
        "batch_size = 128\n",
        "num_epochs = 20\n",
        "\n",
        "# 모델 초기화\n",
        "model = MLP(input_size, hidden_size, output_size)\n",
        "device = torch.device(\"cuda\") # use GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# optimizer, loss function 정의\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# dev_vectors와 dev_data['label']을 텐서 자료형으로 변환\n",
        "dev_tensors = torch.cuda.FloatTensor(dev_vectors.toarray(), device=device)\n",
        "dev_labels = torch.tensor(dev_data['label'], dtype=torch.long, device=device)\n",
        "\n",
        "# 학습 시작 (총 num_epochs 만큼)\n",
        "for epoch in range(num_epochs):\n",
        "  # model을 학습모드로 만든다.\n",
        "  model.train()\n",
        "  \n",
        "  # 학습을 하면서 보고싶은 숫자 (학습이 잘되는지 확인)\n",
        "  epoch_loss = 0\n",
        "  best_accuracy = 0\n",
        "\n",
        "  # train_vectors를 텐서 자료형으로 변환\n",
        "  train_tensors = torch.cuda.FloatTensor(train_vectors.toarray(), device=device)\n",
        "\n",
        "  # batch size 단위로 학습 진행\n",
        "  for i in range(0, len(train_tensors), batch_size):\n",
        "\n",
        "    # batch 단위 데이터 생성\n",
        "    batch_data = train_tensors[i:i+batch_size] # batch size 크기의 데이터가 batch_data\n",
        "    batch_labels = torch.tensor(train_data['label'][i:i+batch_size], device=device)\n",
        "\n",
        "    # 1. 순전파\n",
        "    outputs = model(batch_data)\n",
        "    # 2. 오차 계산\n",
        "    loss = loss_function(outputs, batch_labels)\n",
        "    # 3. 역전파\n",
        "    optimizer.zero_grad()\n",
        "    # 4. 가중치 업데이트\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # epoch loss\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  # 매 epoch마다 dev 성능 측정\n",
        "  # 모델을 평가하는 모드로 셋팅\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    dev_outputs = model(dev_tensors) # dev 데이터의 순전파\n",
        "    dev_preds = torch.argmax(dev_outputs, axis=1)\n",
        "    dev_accuracy = torch.sum(dev_preds == dev_labels).item() / len(dev_labels)\n",
        "\n",
        "    # save best model on dev data\n",
        "    if dev_accuracy > best_accuracy:\n",
        "      best_model = model\n",
        "      best_accuracy = dev_accuracy\n",
        "\n",
        "  print(f\"Epoch {epoch+1}, Accuracy: {dev_accuracy} , loss: {epoch_loss/len(train_tensors)}\")"
      ],
      "metadata": {
        "id": "u_hMc3yACbYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 교차 검증을 사용한 평가\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# all_data = train_data['text']+dev_data['text']+test_data['text']      # all_data = train_data + dev_data + test_data\n",
        "# all_label = train_data['label']+dev_data['label']+test_data['label']  # all_label은 train_data, dev_data, test_date의 레이블을 모두 합한 것 \n",
        "# all_vectors = vectorizer.transform(all_data)    # all_data를 숫자로 변환\n",
        "# scores = cross_val_score(svm, all_vectors, all_label, cv=5)  # 5-cross validation\n",
        "\n",
        "# print(\"All scores:\", scores)  # 전체 점수 출력\n",
        "# print(\"Average: {:.2f}%\".format(scores.mean()*100)) # 다섯 번의 스코어 평균 출력\n",
        "# print(\"Standard deviation: {:.6f}\".format(scores.std()))  # 표준편차 출력 "
      ],
      "metadata": {
        "id": "Np-m5f9DCT1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 그리드 탐색\n",
        "# from sklearn.model_selection import GridSearchCV    \n",
        "\n",
        "# param_grid = [{'max_iter':[500, 1000, 5000], 'C': [1, 10, 100]}]        # 학습 max_iter와 C를 변동 하이퍼파라미터로 설정\n",
        "# grid_search = GridSearchCV(svm, param_grid, cv=3)     # 하이퍼파라미터 서치 할 모델 설정, 각 모델별로 5-cross validation 실행\n",
        "# grid_search.fit(train_vectors, train_data['label'])   # transformed train data로 학습\n",
        "\n",
        "# print(grid_search.cv_results_['mean_test_score'])     # 파라미터 서치 결과\n",
        "# print(grid_search.best_params_)   #최고의 모델 파라미터 \n",
        "\n",
        "# #파라미터 서치 결과 출력\n",
        "# for mean_score, params in zip(grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['params']):\n",
        "#   print(mean_score, params)       "
      ],
      "metadata": {
        "id": "DtH3CLrnCUhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 테스트 세트로 시스템 평가하기\n",
        "from sklearn.metrics import accuracy_score # Accuracy 측정 함수 import\n",
        "final_model = best_model # MLP 모델\n",
        "test_tensors = torch.cuda.FloatTensor(test_vectors.toarray(), device=device)\n",
        "test_outputs = final_model(test_tensors) # 최종 모델로 test 데이터 예측\n",
        "pred_results = torch.argmax(test_outputs, axis=1)\n",
        "accuracy = accuracy_score(test_data['label'], pred_results.tolist()) # 정확도 측정\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100)) # 정확도 출력"
      ],
      "metadata": {
        "id": "caRvzXUtCUrv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}